{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franciscogonzalez-gal/statistical-learning-2/blob/main/Clase8_SL2_Ej.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nombre**: Francisco González\n",
        "\n",
        "**Carnet:** 24002914"
      ],
      "metadata": {
        "id": "YpWyxzarDD-S"
      },
      "id": "YpWyxzarDD-S"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "093c7971-94ba-4f7e-82bf-ccdb484c9b4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "093c7971-94ba-4f7e-82bf-ccdb484c9b4f",
        "outputId": "9b48bbca-1344-4edc-a477-a48e03927a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Bienvenido a 'El Aventurero del Tesoro'!\n",
            "Tu mision es guiar al agente 'A' para encontrar el tesoro 'T' evitando las trampas 'X'.\n",
            "Acciones disponibles: 'arriba', 'abajo', 'izquierda', 'derecha'.\n",
            "Escribe 'fin' para terminar el juego.\n",
            "\n",
            "Estado Actual del Laberinto:\n",
            "      X  \n",
            "  X      \n",
            "    X    \n",
            "X        \n",
            "A   X   T\n",
            "--------------------\n",
            "Elige tu proxima accion (arriba, abajo, izquierda, derecha): arriba\n",
            "\n",
            "Estado Actual del Laberinto:\n",
            "      X  \n",
            "  X      \n",
            "    X    \n",
            "A        \n",
            "S   X   T\n",
            "--------------------\n",
            "Recompensa recibida: -5\n",
            "Recompensa total acumulada: -5.0\n",
            "\n",
            "Caiste en una trampa... Juego terminado.\n",
            "Recompensa final: -5.0\n",
            "--------------------\n",
            "Análisis post-juego: ¿Qué estrategia siguió tu equipo?\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Ejercicio de Aprendizaje por Refuerzo: El Aventurero del Tesoro\n",
        "#\n",
        "# Este archivo Python simula el \"Entorno\" y la interaccion del agente (el equipo de alumnos).\n",
        "# Los alumnos toman decisiones (acciones) y el codigo les devuelve el nuevo estado,\n",
        "# la recompensa y si el episodio ha terminado.\n",
        "# ==============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Definicion del Entorno del juego\n",
        "# 'T': Tesoro (meta)\n",
        "# 'X': Trampa (penalizacion)\n",
        "# ' ': Camino vacio\n",
        "# 'S': Inicio\n",
        "mapa = np.array([\n",
        "    [' ', ' ', ' ', 'X', ' '],\n",
        "    [' ', 'X', ' ', ' ', ' '],\n",
        "    [' ', ' ', 'X', ' ', ' '],\n",
        "    ['X', ' ', ' ', ' ', ' '],\n",
        "    ['S', ' ', 'X', ' ', 'T']\n",
        "])\n",
        "\n",
        "class AmbienteAventurero:\n",
        "    \"\"\"\n",
        "    Esta clase simula el \"Entorno\" del Aprendizaje por Refuerzo, similar a un entorno de IA-Gym.\n",
        "    \"\"\"\n",
        "    def __init__(self, mapa):\n",
        "        self.mapa = mapa\n",
        "        self.estado_inicial = (4, 0)  # Fila 4, Columna 0\n",
        "        self.estado_actual = self.estado_inicial\n",
        "        self.recompensa = 0\n",
        "        self.done = False\n",
        "        self.acciones = {\n",
        "            'arriba': (-1, 0),\n",
        "            'abajo': (1, 0),\n",
        "            'izquierda': (0, -1),\n",
        "            'derecha': (0, 1)\n",
        "        }\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Resetea el entorno a su estado inicial.\n",
        "        \"\"\"\n",
        "        self.estado_actual = self.estado_inicial\n",
        "        self.recompensa = 0\n",
        "        self.done = False\n",
        "        return self.estado_actual, self.done\n",
        "\n",
        "    def step(self, accion):\n",
        "        \"\"\"\n",
        "        El corazon del ciclo de RL.\n",
        "        Recibe una accion, la procesa y devuelve el nuevo estado, la recompensa\n",
        "        y si el episodio ha terminado.\n",
        "        \"\"\"\n",
        "        movimiento = self.acciones.get(accion)\n",
        "        if movimiento is None:\n",
        "            return self.estado_actual, -1, False # Penalizacion por accion invalida\n",
        "\n",
        "        fila_nueva = self.estado_actual[0] + movimiento[0]\n",
        "        col_nueva = self.estado_actual[1] + movimiento[1]\n",
        "\n",
        "        # Verificar limites del mapa\n",
        "        if not (0 <= fila_nueva < len(self.mapa) and 0 <= col_nueva < len(self.mapa[0])):\n",
        "            return self.estado_actual, -0.5, False # Penalizacion por chocar con una pared\n",
        "\n",
        "        self.estado_actual = (fila_nueva, col_nueva)\n",
        "\n",
        "        casilla = self.mapa[self.estado_actual]\n",
        "\n",
        "        if casilla == 'T':\n",
        "            self.recompensa = 10  # Gran recompensa por el tesoro\n",
        "            self.done = True\n",
        "        elif casilla == 'X':\n",
        "            self.recompensa = -5  # Gran penalizacion por la trampa\n",
        "            self.done = True\n",
        "        else:\n",
        "            self.recompensa = -0.1 # Pequena penalizacion por cada paso (costo de tiempo)\n",
        "\n",
        "        return self.estado_actual, self.recompensa, self.done\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Dibuja el mapa para que el equipo pueda ver su progreso.\n",
        "        \"\"\"\n",
        "        mapa_visual = self.mapa.copy().astype('<U1')\n",
        "        fila, col = self.estado_actual\n",
        "        mapa_visual[fila, col] = 'A' # 'A' para el Aventurero\n",
        "\n",
        "        print(\"\\nEstado Actual del Laberinto:\")\n",
        "        for row in mapa_visual:\n",
        "            print(\" \".join(row))\n",
        "        print(\"--------------------\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Bucle de Interaccion del Ejercicio\n",
        "# ==============================================================================\n",
        "def iniciar_juego():\n",
        "    env = AmbienteAventurero(mapa)\n",
        "    estado, done = env.reset()\n",
        "\n",
        "    print(\"¡Bienvenido a 'El Aventurero del Tesoro'!\")\n",
        "    print(\"Tu mision es guiar al agente 'A' para encontrar el tesoro 'T' evitando las trampas 'X'.\")\n",
        "    print(\"Acciones disponibles: 'arriba', 'abajo', 'izquierda', 'derecha'.\")\n",
        "    print(\"Escribe 'fin' para terminar el juego.\")\n",
        "    env.render()\n",
        "\n",
        "    recompensa_total = 0\n",
        "    pasos = 0\n",
        "\n",
        "    while not done:\n",
        "        accion = input(\"Elige tu proxima accion (arriba, abajo, izquierda, derecha): \").lower()\n",
        "        if accion == 'fin':\n",
        "            print(\"Juego terminado por el usuario.\")\n",
        "            break\n",
        "\n",
        "        estado, recompensa, done = env.step(accion)\n",
        "        env.render()\n",
        "        recompensa_total += recompensa\n",
        "        pasos += 1\n",
        "\n",
        "        print(f\"Recompensa recibida: {recompensa}\")\n",
        "        print(f\"Recompensa total acumulada: {recompensa_total:.1f}\")\n",
        "\n",
        "    if 'T' in env.mapa[estado]:\n",
        "        print(f\"\\n¡Felicidades, encontraste el tesoro en {pasos} pasos!\")\n",
        "    elif 'X' in env.mapa[estado]:\n",
        "        print(f\"\\nCaiste en una trampa... Juego terminado.\")\n",
        "\n",
        "    print(f\"Recompensa final: {recompensa_total:.1f}\")\n",
        "    print(\"--------------------\")\n",
        "    print(\"Análisis post-juego: ¿Qué estrategia siguió tu equipo?\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iniciar_juego()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis post-juego: ¿Qué estrategia siguió tu equipo?\n",
        "\n",
        "El equipo no siguió una política clara, ya que la primera acción fue \"arriba\" y llevó directamente a una trampa.  \n",
        "La estrategia fue **miopía**: se pensó en el movimiento inmediato sin considerar el valor futuro ni las posibles consecuencias.  \n",
        "\n",
        "Una mejor estrategia habría sido:  \n",
        "- Evitar movimientos hacia celdas de riesgo inmediato.  \n",
        "- Planificar una ruta hacia el tesoro aceptando pequeñas penalizaciones por pasos.  \n",
        "- Valorar el largo plazo sobre la ganancia inmediata, aplicando la lógica de la **Ecuación de Bellman**.  \n"
      ],
      "metadata": {
        "id": "tXRnSJd1Cnnr"
      },
      "id": "tXRnSJd1Cnnr"
    },
    {
      "cell_type": "markdown",
      "id": "fe6ae787-f2b7-4980-879c-fd013b2e9b71",
      "metadata": {
        "id": "fe6ae787-f2b7-4980-879c-fd013b2e9b71"
      },
      "source": [
        "1. Agente y Entorno:\n",
        "\n",
        "- ¿Quién representaba al Agente en este ejercicio?\n",
        "\n",
        "- ¿Qué elementos del código y el juego representaban el Entorno?\n",
        "\n",
        "2. Estado, Acción y Recompensa:\n",
        "\n",
        "- Describa un Estado en el juego. ¿Qué información te daba?\n",
        "\n",
        "- ¿Cuáles eran las Acciones? ¿Y la Política de tu equipo?\n",
        "\n",
        "- ¿Cómo se usaron las Recompensas para guiar sus decisiones? ¿Qué diferencia hubo entre una recompensa por paso y una recompensa por objetivo?\n",
        "\n",
        "3. La Ecuación de Bellman:\n",
        "\n",
        "- Cuando estaban decidiendo qué hacer, ¿pensaron solo en la recompensa del siguiente paso?\n",
        "\n",
        "- Si encontraron el tesoro, ¿cree que la mejor decisión inmediata fue la que los llevó a él? ¿Podrían haber tomado una penalización pequeña para obtener una gran recompensa después? Esto es el concepto de pensar en el valor a largo plazo, que es el corazón de la Ecuación de Bellman."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Agente y Entorno\n",
        "- **Agente:** el jugador/equipo que decide las acciones.  \n",
        "- **Entorno:** el mapa, las reglas de movimiento, las recompensas y la clase `AmbienteAventurero`.  \n",
        "\n",
        "\n",
        "# 2. Estado, Acción y Recompensa\n",
        "- **Estado:** posición actual del agente en la grilla.  \n",
        "- **Acciones:** arriba, abajo, izquierda, derecha.  \n",
        "- **Política:** estrategia usada para elegir acciones.  \n",
        "- **Recompensas:** penalizan pasos y trampas, premian el tesoro.  \n",
        "\n",
        "\n",
        "\n",
        "# 3. Ecuación de Bellman\n",
        "- **Recompensa inmediata:** puede llevar a errores (ejemplo: caer en trampa).  \n",
        "- **Valor a largo plazo:** aceptar pequeñas penalizaciones para alcanzar la gran recompensa final (tesoro).  \n"
      ],
      "metadata": {
        "id": "TdA62bqXCtro"
      },
      "id": "TdA62bqXCtro"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NaVKa-bRBkv1"
      },
      "id": "NaVKa-bRBkv1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}